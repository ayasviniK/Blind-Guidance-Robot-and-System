# üëÅÔ∏è Vision Assistant - Accessibility Feature

## Overview

The Vision Assistant is a React-based implementation of your Python `gemini_vision_simple.py` script, designed to help blind or visually impaired users navigate using real-time AI-powered scene analysis.

## Features

### ‚úÖ Integrated into Web App

- No need for Python environment
- Runs directly in the browser
- Uses the same ESP32-CAM stream
- Powered by Google Gemini Vision AI

### üéØ Core Functionality

1. **Automatic Scene Analysis**

   - Captures frames from ESP32-CAM feed
   - Analyzes obstacles, people, and hazards
   - Provides distance estimates when possible
   - Focuses on navigation-critical information

2. **Audio Feedback**

   - Text-to-Speech for descriptions
   - Adjustable speech rate (0.5x - 2.0x)
   - Automatic announcements at set intervals
   - Manual repeat function

3. **Customizable Settings**
   - Analysis interval: 3-30 seconds
   - Speech speed control
   - Audio on/off toggle
   - Manual analysis trigger

## How It Works

### Frame Capture Process

```javascript
Video Stream (ESP32)
  ‚Üí Hidden <video> element
  ‚Üí Canvas capture
  ‚Üí Base64 JPEG
  ‚Üí Gemini Vision API
  ‚Üí Text description
  ‚Üí Text-to-Speech
```

### Gemini Vision API Integration

The component uses the **gemini-pro-vision** model for image analysis:

```typescript
// From src/config/gemini.ts
export const analyzeImage = async (imageData: string, prompt: string) => {
  const visionModel = genAI.getGenerativeModel({ model: "gemini-pro-vision" });

  const imagePart = {
    inlineData: {
      data: base64Data,
      mimeType: "image/jpeg",
    },
  };

  const result = await visionModel.generateContent([prompt, imagePart]);
  return result.response.text();
};
```

## Usage

### For End Users

1. **Start the Assistant**

   - Click "Start Vision Assistant" button
   - Grant any browser permissions if prompted
   - Assistant will begin analyzing automatically

2. **Listen to Descriptions**

   - Audio descriptions play at set intervals
   - Focus on immediate obstacles and hazards
   - Includes distance estimates when possible

3. **Manual Analysis**

   - Click "Analyze Now" for immediate feedback
   - Useful when approaching new areas
   - Bypasses interval timer

4. **Adjust Settings**
   - **Analysis Interval**: How often to analyze (3-30s)
   - **Speech Speed**: Faster for experienced users
   - **Audio Toggle**: Mute if using screen reader

### For Developers

#### Add to Your Component

```typescript
import VisionAssistant from "./components/VisionAssistant";

function MyApp() {
  return (
    <VisionAssistant
      streamUrl="http://172.20.10.3:8080/stream"
      onError={(error) => console.error(error)}
    />
  );
}
```

#### Component Props

```typescript
interface VisionAssistantProps {
  streamUrl?: string; // ESP32 stream URL (defaults to .env)
  onError?: (error: Error) => void; // Error callback
}
```

## Comparison with Python Version

| Feature           | Python Version           | React Version             |
| ----------------- | ------------------------ | ------------------------- |
| **Platform**      | Desktop (macOS/Windows)  | Web Browser               |
| **Dependencies**  | OpenCV, PIL, requests    | Built into browser        |
| **Setup**         | Python + packages        | npm install               |
| **Portability**   | Local machine only       | Any device with browser   |
| **UI**            | OpenCV window            | Modern web interface      |
| **Accessibility** | Command line             | Full web accessibility    |
| **Speech**        | macOS `say` command      | Web Speech API            |
| **Frame Capture** | requests + MJPEG parsing | HTMLVideoElement + Canvas |

## Configuration

### Vision Prompt

The AI uses this prompt to generate descriptions:

```javascript
const VISION_PROMPT = `
Describe this scene for a blind person who is walking. 
Be concise and focus on immediate obstacles, people, and potential hazards directly ahead.
Mention distances if possible (e.g., 'a person is about 10 feet away').
Start your description with what is most important for navigation.
Example: 'Person walking towards you, about 5 feet away.' 
Example: 'Stairs going down directly ahead.'
Example: 'Clear path ahead.'
`;
```

You can customize this in `src/components/VisionAssistant.tsx`

### Default Settings

```typescript
const DEFAULT_SETTINGS = {
  analysisInterval: 10, // seconds
  speechRate: 1.2, // 1.0 = normal speed
  audioEnabled: true,
};
```

## Browser Compatibility

| Feature        | Chrome | Edge | Safari | Firefox |
| -------------- | ------ | ---- | ------ | ------- |
| Video Capture  | ‚úÖ     | ‚úÖ   | ‚úÖ     | ‚úÖ      |
| Canvas API     | ‚úÖ     | ‚úÖ   | ‚úÖ     | ‚úÖ      |
| Text-to-Speech | ‚úÖ     | ‚úÖ   | ‚úÖ     | ‚úÖ      |
| Gemini API     | ‚úÖ     | ‚úÖ   | ‚úÖ     | ‚úÖ      |

**All modern browsers fully supported!**

## API Usage & Costs

### Gemini API Calls

- **Model**: `gemini-pro-vision`
- **Frequency**: Based on interval setting (default: 10s)
- **Data**: Base64 JPEG image + text prompt

### Estimated Costs (Google AI Studio)

At 10-second intervals:

- **Per Hour**: ~360 API calls
- **Per Day**: ~8,640 API calls

Check current Gemini API pricing: https://ai.google.dev/pricing

### Optimization Tips

1. **Increase Interval**: 15-20s reduces API calls
2. **Manual Mode**: Disable auto-analysis, use button only
3. **Motion Detection**: Only analyze when robot is moving
4. **Scene Change Detection**: Only analyze when scene changes significantly

## Troubleshooting

### No Video Stream

**Problem**: Video element not loading
**Solutions**:

- Check ESP32 is streaming at correct URL
- Verify `VITE_ROBOT_STREAM_URL` in `.env`
- Test stream URL directly in browser
- Check network connectivity

### Analysis Not Running

**Problem**: No descriptions being generated
**Solutions**:

- Check Gemini API key in `.env`
- Open browser console for errors
- Verify `gemini-pro-vision` model is available
- Check API quota/billing

### Audio Not Playing

**Problem**: Text-to-Speech not working
**Solutions**:

- Check audio is enabled in component
- Verify browser supports Web Speech API
- Check system volume
- Try different browser (Chrome/Edge recommended)

### Slow Analysis

**Problem**: Long delay between captures
**Solutions**:

- Reduce analysis interval
- Check internet connection speed
- Monitor Gemini API response times
- Consider using smaller image resolution

## Accessibility Features

### Screen Reader Support

- All controls have proper ARIA labels
- Status indicators announced
- Keyboard navigation supported

### High Contrast Mode

- Compatible with browser high contrast settings
- Large, clear buttons
- Distinct status indicators

### Keyboard Shortcuts

- `Space`: Start/Stop assistant
- `Enter`: Analyze now
- `Escape`: Stop speaking

## Advanced Customization

### Custom Analysis Prompts

Create different prompts for different scenarios:

```typescript
const PROMPTS = {
  navigation: "Describe obstacles and hazards for navigation...",
  shopping: "Describe products and aisles in this store...",
  reading: "Read any text visible in this image...",
};
```

### Frame Pre-processing

Add image enhancement before analysis:

```typescript
const enhanceFrame = (canvas: HTMLCanvasElement) => {
  const ctx = canvas.getContext("2d");
  // Increase contrast
  ctx.filter = "contrast(1.2) brightness(1.1)";
  // Apply other filters as needed
};
```

### Multi-language Support

Change speech synthesis language:

```typescript
const utterance = new SpeechSynthesisUtterance(text);
utterance.lang = "es-ES"; // Spanish
utterance.lang = "fr-FR"; // French
utterance.lang = "de-DE"; // German
```

## Future Enhancements

### Planned Features

- [ ] Object detection with bounding boxes
- [ ] Distance estimation using depth camera
- [ ] Facial recognition for familiar people
- [ ] Text recognition (OCR) for signs
- [ ] Path prediction and planning
- [ ] Offline mode with local AI model
- [ ] Mobile app version
- [ ] Multi-camera support

### Integration Ideas

- Connect with smart glasses
- Integration with navigation APIs
- Emergency alert system
- Location-based contextual help
- Social features (share routes)

## Testing

### Test Checklist

- [ ] Start/Stop functionality
- [ ] Automatic analysis at intervals
- [ ] Manual analysis button
- [ ] Audio feedback working
- [ ] Speech rate adjustment
- [ ] Interval setting changes
- [ ] Video stream loading
- [ ] Error handling
- [ ] Browser console clear of errors
- [ ] Accessibility with screen reader

### Test Scenarios

1. **Indoor Navigation**

   - Detect furniture
   - Identify doorways
   - Warn about steps

2. **Outdoor Navigation**

   - Detect pedestrians
   - Identify crosswalks
   - Warn about vehicles

3. **Edge Cases**
   - Low light conditions
   - Bright sunlight
   - Crowded scenes
   - Fast movement

## Support & Resources

- **Gemini API Docs**: https://ai.google.dev/docs
- **Web Speech API**: https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API
- **Accessibility Guidelines**: https://www.w3.org/WAI/WCAG21/quickref/

## License

MIT License - Same as main project

---

**Created**: January 2025  
**Version**: 1.0.0  
**Status**: ‚úÖ Production Ready
